---
layout: page
title: Lab 2 ðŸ¦†ðŸ¦†
importance: 2
category: CMPUT 401 - Robotics ðŸ¦†
---

<b> Objective </b> <br>
<p> Our objective is to practice using Python containers to drive the robot following predefined sequence of commands and getting familiars with ROS process communication concepts e.g. messages and services through coding. </p>


<b> Background </b> <br>
<p> Since the previous exercise, we have calibrated our duckiebot and obtained the wheel radius from the wheel calibration file. We drove the robot using dts keyboard control and were able to run our own containers remotely on the robot. Now we would like it to be able to locate itself and drive to the target autonomously, as well as using its LED lights to display its current state. The links in the exercise introduced us to different ROS concepts and examples of writing publisher, subscriber and services, which we used as a starting point. </p>

<b> Methods and Results </b> <br>
<p> To drive the duckiebot we would use various types of publishers and subscribers. For example, to view the camera image we need to subscribe to the compressed image topic published by the duckiebot camera node. To control the motor to start/stop and steer the robot we need to send wheel commands using a publisher. <p>

<b> Hello World Topic </b> <br>
<p> We start by writing the simplest hello world publisher and subscriber. In ROS, publishers and subscribers on a particular topic can share a message type. We followed the duckietown manual example and used the String message type to communicate between a publisher and a subscriber. As we verified, after starting the program, the publisher sends the string 'Hello world' to topic 'chatter' every second and the latter listens to the same topic and echoes the string to stdout.
Publishing to an Image Topic </p>

<p> To understand how camera images are published in ROS, we opened dts start_gui_tools and used rqt_graph and rostopic list commands to investigate what kinds of topics are being published on the robot, and then use rostopic type command to find out the message type of the messages on that particular topic. After knowing the message type, we can then look up the std_msgs or duckietown_msgs documentations on how to read/write such messages.  </p>

<p> Following these steps, we found out the camera input is published as CompressedImage over the hostname/camera_node/input/compressed topic. We then follow ROS example code of publishing and subscribing to CompressedImage topic to write our own image topic, which consists of a subscriber that subscribes to the camera node and a publisher that republishes the images a few times every second. To verify the images are indeed published, we opened rqt_image_view and visually verified that the robot camera input is displayed in real-time on our custom topic. </p>

<b> Straight Line/ Rotation Task </b> <br>
<p> The next goal is to test basic driving operations. First we looked for topics to control the robot and to estimate its position through number of ticks traveled by its wheels and found 'hostname/wheels_driver_node/wheels_cmd' as well as 'hostname/left(/right)_wheel_encoder_node/tick'. With the ticks data from left and right wheel we can perform odometry through dead reckoning, which in this case is done by integrating the robot's position while it's moving each time it receives a message from either wheels: <br>
def update_robot_pose(dl, dr):  # dl and dr are number of ticks advanced in the two wheel since the last update <br>
    	<blockquote> distance = (dleft + dright) / 2 <br>
    	 dtheta = (dright - dleft) / self.radius / 2 <br>
        x += distance * math.cos(self.theta)  # x unit is in ticks, need to be converted to meters later <br>
    	y += distance * math.sin(self.theta) <br>
    	theta += dtheta. </blockquote>
</p>

<p> We keep track of the left and right wheel encoder ticks and compute the difference to get the change in wheel ticks, then compute the theta and update the robot position in the world frame. We script the robot to move 1.25 meters forward, and turn 90 degrees right four times. To turn the robot, we set the speed of each wheel to the same number with opposite signs. </p>
<p>
Afterwards we measured the actual distance traveled, and saw if the turning angles were accurate. After tuning the parameters wheel_radius and meters_per_wheel_tick several times and measuring the wheel's radius physically with a ruler, we got relatively satisfactory results.
<p>
We moved on to the second part of the exercise. <br><br>
<b> First Step in the Second Part: Writing an LED Service Client </b> <br>
<p>
The way we will change the color of the LED is by having a client container to send color change requests to a server container which talks to the LED driver and carry out the change. After searching online we found the source code of GitHub dt-core/.../led_emitter_code.py. By reading the code, we know led_emitter_node can act as such a server, which receives ChangePattern requests and change the color of the LED. We then added our client code to our script, which initializes a rospy.ServiceProxy object and we used it to send a ChangePattern request every time we want to change the color. </p>
<b> Rosbag Recording </b> <br>
<p> To reproduce any error we encounter, we record a rosbag of all the messages sent over the ROS message bus when the script is running. Following the manual, we used start_gui_tools and rosbag record -a to start recording the bag and ctrl+c to stop the recording. With the bag file obtained, we sent it from the start_gui_tools container to the duckiebot and then to our computers by two scp commands. On our own computer, we were able to start analyzing the information in the bag after installing the rosbag package for Python on our computers. </p>
<b>Driving around the Duckietown</b><br>
<p> As the final step, we scripted our robot to go around a square path and back to the original position, then go around in a circle and go back to the original position again, as well as indicate its progress of execution by the color of the LED. <br> We broke down the tasks into subcommands: <br>
Turn the robot to target angle theta in the world frame. <br>
Move to x, y position in the world frame <br>
Change LED color to red (state 1), white (state 2), blue (state 3) and green (state 4) <br>
Stop movement <br>
To make the movement as accurate as we could, we used a simple PID controller during forward movement (with only the proportional term) to account for angle error resulted from inaccurate turning or drifting while driving between points. Combining everything, we started the program after starting the led_emitter_node container and the rosbag recording.  </p>

<b> Analyzing the Rosbag </b> <br>
<p> Following the duckietown manual section 'Working with Log', we were able to retrieve the messages in the bag file through iterating over the messages in the rosbag and filter for the topics we look for. We used matplotlib to plot the location of the robot for each time step, and then used OpenCV to display the image at each step in the form of a video. </p>

<b>Part 1 Obstacles</b><br>
<p>For part 1 image publisher, we first named our custom image topic hostname/MyCompressedImage but saw no image published in rqt_image_view(though rqt_image_view is able to display the topic name). After renaming the topic to HOST_NAME/compressed instead the issue is fixed and we start to receive the images.</p>
<p>The robot seems to ignore some of the wheel commands when the program just boots up, so we decided to add some waiting at the start of the script before the robot starts moving to solve the issue. </p>
<p>When reading the wheel encoder data, we were initially confused by the unit of measurement at first. We asked the TA and realized that the wheel encoder records data in ticks rather than in meters. However, even though we later converted one tick to 1/135 of a full circle, the robot still has significant error in estimating its distance traveled. After some investigation we found out its calibrated wheel radius is much smaller than the actual wheel radius measured with a ruler. Fixing the radius seemed to bring down the relative error within a few percents. </p>
<b> Part 2 Obstacles <b><br>
<p> We had the issue of the container not automatically shutting down, so we added the rospy.signal_shutdown call at the end of the script to make sure the program is properly shut down after the task. <br>
The biggest obstacle from this part comes from interpreting the wheel encoder data, as the messages published on the two topics have unreliable timestamps. Our first approach is to take the timestamp on the message as the true time when the sensor records the data and update the robot position as follows (assuming the last update is at time t0, and since then we received a message from left wheel at time t1 and a message from right wheel at time t2 with t2 > t1 > t0): <br>
<blockquote>
newleft = left_reading_at_t1 <br>
newright = last_right_reading + (right_reading_at_t2 - last_right_reading) * (t1 - t0) / (t2 - t0) <br>
newtime = t1 <br>
dleft = newleft - last_left_reading <br>
dright = newright - last_right_reading <br>
update_robot_pose(dleft, dright) <br>
t0 = newtime <br>
</blockquote>
However, we quickly found that the robot turned one way when it's driving forward. By analyzing the rosbag, we found that it was because while the messages from both wheels publish at about the same rates, the order got mixed up in the ROS message bus and timestamps on the messages seemed to only be applied after their order getting mixed up and was therefore unreliable as well. </p>

<p> We also noticed that despite the order being mixed up, the messages seem to be sent in a relatively constant rate, and both wheels sent about the same number of messages (930 vs. 940 in one of the test recordings) in the same time frame. This observation allowed us to use the following method to approximate the robot position: We assume that a message from the left wheel is always paired up with a message from the right wheel at some point. Even though in theory the approximate can have larger errors as the time goes if the two wheels desync, in testing this seemed to significantly improve the accuracy of driving. </p>
<p>We also had some trouble plotting the data. When recalculating the robot pose from the wheel encoder data in the rosbag, we see the robot turns -4 radians at the start of the task which does not correspond to the 90 degrees turn we observed. We failed to recreate the same results that we see published on the pose2d topic. We believe that this could come from the difference in the messages received by our script and by the rosbag recording. </p>


<div class="row justify-content-md-center">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/404_lab2.png" title="Theta" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1. Plot of theta if recalculating it from the wheel encoder data in rosbag
</div>

<b> QAs </b><br>
How do you convert the location and theta at the initial robot frame to the world frame? <br>
<p>
The robot frame has both a rotation dtheta = pi / 2 and an offset dp = (0.32, 0.32). We can transformed the point Pr = (xr, yr, thetar) to world frame using: <br>
<blockquote> xl = cos(dtheta)xr - sin(dtheta)yr + dpx <br>
yl = sin(dtheta)xr + cos(dtheta)yr + dpy <br>
thetal = thetar + dtheta <br>
</blockquote></p>

Can you explain why there is a difference between actual and desired location? <br>
<p> It could be our parameters are not tuned perfectly, and that the physical sensors are not perfectly accurate as well. Also at higher speed more of the odometry discrepancy may come from the momentum of the robot (as we assume the robot starts/stops instantly when the command is sent) or from slipping wheels in contact with the ground. 
For example, we found that the robot turns too fast when it's off the duckietown on the classroom floor, which could be that the classroom floor is more slippery. </p>

Which topic(s) did you use to make the robot move? How did you figure out the topic that could make the motor move? <br>
<p>'wheels_cmd' for control and '...wheel_encoder_node/tick' for odometry. See the methods section for details. </p>

Which speed are you using? What happens if you increase/decrease the speed? <br>
<p>Changing the robot speed seemed to make the turning angles inaccurate, since the turning angles parameter is tuned over only a fixed speed. For this reason we avoided changing the speed of our robot while carrying out the final task. We used 0.6 as our speed. If we went lower, the vehicle doesnâ€™t move because of low motor output, and if we set a higher speed, we get a sharper, less accurate angle upon turning. </p>

How did you keep track of the angle rotated? <br>
<p>See the pose update formulate in the methods section.</p>

Which topic(s) did you use to make the robot rotate? <br>
<p>'wheels_cmd' for control and '...wheel_encoder_node/tick' for odometry. See the methods section for details. </p>

How did you estimate/track the angles your DuckieBot has travelled? <br>
<p>See the pose update formulate in the methods section.</p>

Print the total execution time for your entire task and the final location with respect to the world frame. <br>
<p>Final reading on the robot:<br>
x = 0.018 y = -0.059<br>
Execution time: 47.65644907951355 seconds <br></p>

What is the final location of your robot as shown in your odometry reading? Is it close to your robotâ€™s actual physical location in the mat world frame? <br>
<p>The final location of the robot according to the odometey readings is: <br>
x = - 0.075 m, y = - 0.020 m <br>
The actual physical location of the robot was x = 0.14 m and y= 0.05 m <br>
It is close to its odometry location, which is about 0.20m from its actual location.</p>

<video width="320" height="240" controls>
  <source src="412_lab2_part2.mOV" type="video/mov">
Your browser does not support the video tag.
</video>

<b>Division of work:</b><br>
<p>Tianming - worked on most of part 1 and 2, including the image topic publisher, the ros bag analyzer, the position integration. The straight line, rotation, the measurement and the LED task. <br>
Leen - Worked on most of part 2, including the measurement, and the LED light task. 
</p>
<b> References: </b> <br>
ROS concepts (nodes, topics, services): http://wiki.ros.org/ROS/Concepts <br>
Duckietown message and service types: https://github.com/duckietown/dt-ros-commons/tree/daffy/packages/duckietown_msgs <br>
Message types in ROS std_msgs: http://wiki.ros.org/std_msgs <br>
Sensor message types: http://wiki.ros.org/sensor_msgs <br>
Publishing and Subscribing CompressedImage: http://wiki.ros.org/rospy_tutorials/Tutorials/WritingImagePublisherSubscriber <br>
ROS template github repo: https://github.com/duckietown/template-ros <br>
Duckietown manual on publishers and subscribers: https://docs.duckietown.org/daffy/duckietown-robotics-development/out/dt_infrastructure.html <br>
Publisher naming issue: https://answers.ros.org/question/270881/unable-to-load-plugin-for-transport-image_transportcompressed_sub/ <br>
LED emitter node: https://github.com/duckietown/dt-core/blob/daffy/packages/led_emitter/src/led_emitter_node.py <br>
Odometry with Wheel Encoder: https://docs.duckietown.org/daffy/duckietown-robotics-development/out/odometry_modeling.html <br>
Rosbag reading and recording: https://docs.duckietown.org/daffy/duckietown-robotics-development/out/ros_logs.html <br>
ROS tutorial on writing the client node: http://wiki.ros.org/ROS/Tutorials/WritingServiceClient%28python%29 <br>
Converting a pyplot plot to numpy array: https://stackoverflow.com/questions/7821518/matplotlib-save-plot-to-numpy-array <br>
Cheat sheet google doc: https://docs.google.com/document/d/1bQfkR_tmwctFozEZlZkmojBZHkegscJPJVuw-IEXwI4/edit <br>



